{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paidy - Data Science Technical Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any project at scale, there is always a need to load data from files or various other sources into databases, which can be relational databases or big data storages like Hive or Impala. The reason for this is companies are becoming data centric and Data Professionals are at the centre of that. The Data needs to have all the 6 Vs value which can be harnessed by the organization. The 6 Vs are as follows:\n",
    "\n",
    "### Volume\n",
    "Volume, the first of the 5 V's of big data, refers to the amount of data that exists. Volume is like the base of big data, as it is the initial size and amount of data that is collected. If the volume of data is large enough, it can be considered big data. What is considered to be big data is relative, though, and will change depending on the available computing power that's on the market.\n",
    "\n",
    "### Velocity\n",
    "The next of the 5 V's of big data is velocity. It refers to how quickly data is generated and how quickly that data moves. This is an important aspect for companies need that need their data to flow quickly, so it's available at the right times to make the best business decisions possible.\n",
    "\n",
    "An organization that uses big data will have a large and continuous flow of data that is being created and sent to its end destination. Data could flow from sources such as machines, networks, smartphones or social media. This data needs to be digested and analyzed quickly, and sometimes in near real time.\n",
    "\n",
    "As an example, in healthcare, there are many medical devices made today to monitor patients and collect data. From in-hospital medical equipment to wearable devices, collected data needs to be sent to its destination and analyzed quickly.\n",
    "\n",
    "In some cases, however, it may be better to have a limited set of collected data than to collect more data than an organization can handle -- since this can lead to slower data velocities.\n",
    "\n",
    "### Variety\n",
    "The next V in the five 5 V's of big data is variety. Variety refers to the diversity of data types. An organization might obtain data from a number of different data sources, which may vary in value. Data can come from sources in and outside an enterprise as well. The challenge in variety concerns the standardization and distribution of all data being collected.\n",
    "\n",
    "Collected data can be unstructured, semi-structured or structured in nature. Unstructured data is data that is unorganized and comes in different files or formats. Typically, unstructured data is not a good fit for a mainstream relational database because it doesn't fit into conventional data models. Semi-structured data is data that has not been organized into a specialized repository but has associated information, such as metadata. This makes it easier to process than unstructured data. Structured data, meanwhile, is data that has been organized into a formatted repository. This means the data is made more addressable for effective data processing and analysis.\n",
    "\n",
    "### Veracity\n",
    "Veracity is the fourth V in the 5 V's of big data. It refers to the quality and accuracy of data. Gathered data could have missing pieces, may be inaccurate or may not be able to provide real, valuable insight. Veracity, overall, refers to the level of trust there is in the collected data.\n",
    "\n",
    "Data can sometimes become messy and difficult to use. A large amount of data can cause more confusion than insights if it's incomplete. For example, concerning the medical field, if data about what drugs a patient is taking is incomplete, then the patient's life may be endangered.\n",
    "\n",
    "### Visualize \n",
    "Visualize component leads to storyboarding and how beautifully the data can express a flow of patterns and trends which cannot be seen with naked eyes looking at numbers.\n",
    "\n",
    "\n",
    "### Leading To\n",
    "\n",
    "\n",
    "### Value\n",
    "The last V in the 6 V's of big data is value. This refers to the value that big data can provide, and it relates directly to what organizations can do with that collected data. Being able to pull value from big data is a requirement, as the value of big data increases significantly depending on the insights that can be gained from them.\n",
    "\n",
    "Organizations can use the same big data tools to gather and analyze the data, but how they derive value from that data should be unique to them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "However, Python only works in-memory for a single node process. While distributed programming languages have tried to face this challenge, they are still generally in-memory and can never hope to process all of your data, and moving data is expensive. On top of all of this, data scientists must also find convenient ways to deploy their data and models. The whole process is time consuming. That is why we come up with efficient ways for Data Ingestion and performing manipulation and exploratory Analytics on it.\n",
    "\n",
    "Belows lets deep dive into the assignment and see what we can do out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Ingestion\n",
    "\n",
    "The first part of this exercise requires you to build a PoC for a data ingestion system to make incoming CSV data easy to use / query by our Data Scientists. You can find the data in this repository:\n",
    "\n",
    "    data_dictionary.md - A variable dictionary is provided with definitions for each variable.\n",
    "    sample_data.csv - The dataset is provided to you in the form of a CSV file which can be found in this repository.\n",
    "\n",
    "You have the freedom to choose how you go about building the PoC but here are a few guidelines:\n",
    "\n",
    "    You should expect to receive files with data (assume the same format) so your solution should be able to ingest them on a regular basis (e.g. every hour or day).\n",
    "    The data should be stored in a central place and accessible/readable by multiple data scientists (even in parallel).\n",
    "    It is up to you to choose the underlying data storage/compute engine/database you use, but the data should also be accessible via SQL, Python and/or R.\n",
    "    \n",
    "\n",
    "### NOTE\n",
    "Before beginning the assignment i would like to make a clear problem statement, that will help all the process stakeholders understand the problem, the entities it is affecting and what can it lead to if not tackled and the overall harm to the value.\n",
    "\n",
    "I use a simple breakdown for creating problem statements. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem Statement\n",
    "\n",
    "The three primary characteristics of Problem statement must be : Relevant, Specific and Unambigious.\n",
    "\n",
    "The Problem Statemnt for the coding challenge would be like this which highlights and covers all the points in a crisp statement:\n",
    "\n",
    "\n",
    "Step 1: Begin by understanding the businessâ€™s vision.\n",
    "\n",
    "Step 2: What are the pain point that they are facing?\n",
    "\n",
    "Step 3:What resources are available?\n",
    "\n",
    "Step 4: What are the potential benefits?\n",
    "\n",
    "Step 5: What risks are there in pursuing the project?\n",
    "\n",
    "Step 6: Determine if the expected benefits are realistic and attainable from a data point of view.\n",
    "\n",
    "\n",
    "#### The Problem Data Ingestion and Descriptive Analytics has the impact of untimed ETL, unavailability of data across organization which affects the Data Scientists and Product Engineers to understand the data and get a hand on it, so a good starting point would be starting with Data Pipelines and Decriptive Statistics so information can be passed on to them using various tools and data is made available with richer meta-data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries---Please download the libraries which are missing to run this notebook smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "from enum import Enum, auto\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"sample_data.csv\",index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DataDictionary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Framework for Data Ingestion\n",
    "\n",
    "Following is the code developed for generic Data Ignestion and as pysql alchemy supports different kind of engine, we can ingest data into different kind of DBs just we will have to write some classes and constructors for the same.\n",
    "\n",
    "\n",
    "#### NOTE : Please make sure to start the docker container using docker compose up pgsql command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "def auto_str(cls):\n",
    "    \"Auto generate __str__\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s(%s)\" % (\n",
    "            type(self).__name__,\n",
    "            \", \".join(\"%s=%s\" % item for item in vars(self).items()),\n",
    "        )\n",
    "\n",
    "    cls.__str__ = __str__\n",
    "    return cls\n",
    "\n",
    "#export\n",
    "class GetAttr:\n",
    "    \"Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`\"\n",
    "    _default='default'\n",
    "    def _component_attr_filter(self,k):\n",
    "        if k.startswith('__') or k in ('_xtra',self._default): return False\n",
    "        xtra = getattr(self,'_xtra',None)\n",
    "        return xtra is None or k in xtra\n",
    "    def _dir(self): return [k for k in dir(getattr(self,self._default)) if self._component_attr_filter(k)]\n",
    "    def __getattr__(self,k):\n",
    "        if self._component_attr_filter(k):\n",
    "            attr = getattr(self,self._default,None)\n",
    "            if attr is not None: return getattr(attr,k)\n",
    "        raise AttributeError(k)\n",
    "    def __dir__(self): return custom_dir(self,self._dir())\n",
    "    def __setstate__(self,data): self.__dict__.update(data)\n",
    "        \n",
    "\n",
    "#export\n",
    "class ObjectFactory():\n",
    "    \"Generic object factory\"\n",
    "    def __init__(self):\n",
    "        self._builders = {}\n",
    "\n",
    "    def register_builder(self, key, builder):\n",
    "        self._builders[key] = builder\n",
    "\n",
    "    def create(self, key, **kwargs):\n",
    "        builder = self._builders.get(key)\n",
    "        if not builder:\n",
    "            raise ValueError(key)\n",
    "        return builder(**kwargs)\n",
    "\n",
    "#export\n",
    "class DbTargetProvider(ObjectFactory):\n",
    "    \"Database provider\"\n",
    "\n",
    "    def get(self, id, **kwargs):\n",
    "        \"\"\"Create the database interface\"\"\"\n",
    "        return self.create(id, **kwargs)\n",
    "    \n",
    "\n",
    "#export \n",
    "class FileSourceProvider(ObjectFactory):\n",
    "    \"Supported file sources\"\n",
    "\n",
    "    def get(self, id, **kwargs):\n",
    "        \"\"\"Create the file interface\"\"\"\n",
    "        return self.create(id, **kwargs)\n",
    "\n",
    "    #export\n",
    "class DatabaseTarget(Enum):\n",
    "    PostgreSQL = auto()\n",
    "\n",
    "    \n",
    "\n",
    "#export\n",
    "class FileSource(Enum):\n",
    "    CSV = auto()\n",
    "\n",
    "    \n",
    "#export\n",
    "class PgSqlDbBuilder:\n",
    "    \"\"\"PostgreSQL database builder.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._instance = None\n",
    "\n",
    "    def __call__(self, host, port, db, user, password, **_ignored):\n",
    "        if not self._instance:\n",
    "            self._instance = PgSqlDb(\n",
    "                host,\n",
    "                port,\n",
    "                db,\n",
    "                user,\n",
    "                password\n",
    "            )\n",
    "        return self._instance\n",
    "\n",
    "@auto_str\n",
    "class PgSqlDb:\n",
    "    \"\"\"PostgreSQL database destination.\"\"\"\n",
    "\n",
    "    def __init__(self, host, port, db, user, password):\n",
    "        self._host = host\n",
    "        self._port = port\n",
    "        self._db = db\n",
    "        self._user = user\n",
    "        self._password = password\n",
    "\n",
    "    def get_engine(self):\n",
    "        \"\"\"Create and return sqlalchemy engine.\"\"\"\n",
    "        return create_engine(self.get_conn_str())\n",
    "\n",
    "    def get_conn_str(self):\n",
    "        \"\"\"Return the connection string.\"\"\"\n",
    "        return f\"postgresql+psycopg2://{self._user}:{self._password}@{self._host}:{self._port}/{self._db}\"\n",
    "    \n",
    "def create_csv_file_source(file_path, **args):\n",
    "    \"\"\"Create CSV file source.\"\"\"\n",
    "    return CSVSource(file_path, **args) \n",
    "    \n",
    "class CSVSource:\n",
    "    \"\"\"CSV file source.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, **args):\n",
    "        self._file_path = file_path\n",
    "        self._args = args\n",
    "        \n",
    "    def filepath(self):\n",
    "        return self._file_path\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Read the file and return a `DataFrame`\"\"\"\n",
    "        return pd.read_csv(self._file_path, engine=None, **self._args,index_col='Unnamed: 0')\n",
    "    \n",
    "    \n",
    "\n",
    "#export\n",
    "\n",
    "# Register supported database providers\n",
    "db_targets = DbTargetProvider()\n",
    "db_targets.register_builder(DatabaseTarget.PostgreSQL, PgSqlDbBuilder())\n",
    "\n",
    "\n",
    "#export\n",
    "\n",
    "# Register supported file types\n",
    "file_sources = FileSourceProvider()\n",
    "file_sources.register_builder(FileSource.CSV, create_csv_file_source)\n",
    "\n",
    "def ingest(file_source, target_db, table_name, *, if_exists='append', method='multi', schema=None):\n",
    "    \"\"\"Ingest the file into the database table.\"\"\"\n",
    "    \n",
    "    # Create db engine\n",
    "    engine = target_db.get_engine()\n",
    "\n",
    "    # Inspect the target table schema\n",
    "    inspector = inspect(engine)\n",
    "    dtypes = {}\n",
    "    for column in inspector.get_columns(table_name, schema=schema):\n",
    "        dtypes[column[\"name\"]] = column[\"type\"]\n",
    "    logging.info(dtypes)\n",
    "\n",
    "    # Load the excel into database\n",
    "    df = file_source.get_data()\n",
    "    df.to_sql(\n",
    "        table_name, engine, if_exists=if_exists, method=method, chunksize=500, index=False, dtype=dtypes\n",
    "    )\n",
    "\n",
    "    # TODO - Validation\n",
    "    print(f\"\\nTotal records in {file_source.filepath()} - {len(df)}\")\n",
    "    for c in df.columns:\n",
    "        print(f\"{c} - {df[c].nunique()}\")\n",
    "\n",
    "\n",
    "# Create a CSV file source\n",
    "csv_source = file_sources.get(FileSource.CSV, file_path=\"sample_data.csv\")\n",
    "csv_source.get_data()\n",
    "\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'db': 'paidydb',\n",
    "    'user': 'paidy',\n",
    "    'password': 'paidy'\n",
    "}\n",
    "pgsql_target = db_targets.get(DatabaseTarget.PostgreSQL, **config)\n",
    "pgsql_target.get_conn_str()\n",
    "\n",
    "# Ingest to PostgreSQL\n",
    "ingest(csv_source, pgsql_target, 'accounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make this repetitve, we can convert this notebook to .py file using python library and on server we can run every x Hours/Minutes. The only dependency is that the Data Format has to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding the Data\n",
    "\n",
    "A big part of our work is helping data scientists understand the data and build data structures that simplify their work. So itâ€™s important to understand the data for ourselves as well.\n",
    "\n",
    "For the 2nd part of this exercise weâ€™d like for you to give a short presentation (10-15 minutes) describing the data in a way that would be relevant for data scientists.\n",
    "\n",
    "A few guidelines for this part:\n",
    "\n",
    "    Assume that the data scientists are completely unfamiliar with the new data and havenâ€™t heard of it before\n",
    "    Please take this as far as youâ€™d like, but note that you are not expected to train a Machine Learning model or come up with a credit policy.\n",
    "    \n",
    "    You can use whatever tools you prefer for this.\n",
    "    \n",
    "    Beautiful visualizations are great but descriptive summary tables are also great.\n",
    "    \n",
    "    Your exploration of the data does not have to be strictly about credit risk or even finance. If you find something interesting, weâ€™d like to know about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just to be sure we have ingested the data correctly in our DB, we are fetching the data from DB below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_sql(\"select * from accounts\", pgsql_target.get_conn_str());\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFrame.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- There are alot of missing values in MonthlyIncome and Number of Dependents, Imputations will be performed which will be expalined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.select_dtypes('int64').describe().transpose()[['min', '25%', '50%', '75%', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---All integer features are count features as the only binary column is the target SeriousDlqin2yrs\n",
    "\n",
    "---NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse and NumberOfTimes90DaysLate seem to be slightly sparse so in general people do not tend to have past due.\n",
    "\n",
    "---Age has one or more outlier values as 0 is the minimum of the entries and by credit standard minimum years is 18.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.select_dtypes('float64').describe().transpose()[['min', '25%', '50%', '75%', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---The concerning point here is Debt Ratio, as it is a ratio it can only be between 0-1 and values here are as high as 300k. Outliers are definetely present.\n",
    "\n",
    "---Number of Dependents is float where as it should be integer, typecasting will be required to convert it to relevant data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations and Preprocessing\n",
    "##### NOTE: \n",
    "We are using Plotly and it is process heavy,  but the visualizations are interactive and beautiful, so please clear cells after every visualization\n",
    "\n",
    "### Age Feature\n",
    "The first feature that we will be cleaning and imputing is the age feature, as we can see from summary staistics above, the minimum of the age feature was 0 which is practically impossible in the world of credits and loans, as you need to be of a maturity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df = dataFrame\n",
    "fig = px.histogram(df, x=\"age\", color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"box\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that from here, there is one record with 0 age , so we will clean it and repalce it with the median of the data, i would not clean the maximum age outliers, as they can be real but still a bit skeptical about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame['age'] = dataFrame['age'].apply(lambda x: int(dataFrame['age'].median()) if x<18 else x)\n",
    "df = dataFrame\n",
    "fig = px.histogram(df, x=\"age\", color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"box\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more interesting thing which we can observe here is that majority of the data points who have \"Serious Dlqin 2 years - Person experienced 90 days past due delinquency or worse \" are a bit younger than the general age of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Dependents\n",
    "The feature that we will be cleaning and imputing is the number of dependents feature, as we can see from summary staistics above, the Number of Dependents feature have a lot of missing values so we will take a deeper look into into and try and understand which imputation method will be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=\"NumberOfDependents\", color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[['NumberOfDependents']].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About 2% of the data is missing so and the data is a bit skewed, so the best central tendency to impute data would be median and then convert the data to integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['NumberOfDependents'].fillna(dataFrame['NumberOfDependents'].median(), inplace=True)\n",
    "dataFrame['NumberOfDependents'] = dataFrame['NumberOfDependents'].astype('int64')\n",
    "df = dataFrame\n",
    "fig = px.histogram(df, x=\"NumberOfDependents\", color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Past Days Feature \n",
    "\n",
    "Deep diving into the apst Day Feature, we will see that corelations are very high, in general they should be high, but in this case theya re very high so we will dig more into it, as well there are some values which are duplicates in the columns which we will see and figure how to eliminate them if there are any.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=np.log1p(df[\"NumberOfTime30-59DaysPastDueNotWorse\"]), color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "\n",
    "fig.show()\n",
    "fig = px.box(df, y=\"NumberOfTime30-59DaysPastDueNotWorse\", x=\"SeriousDlqin2yrs\")\n",
    "fig.update_layout(yaxis_range=[0,20])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=np.log1p(df[\"NumberOfTime60-89DaysPastDueNotWorse\"]), color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "\n",
    "fig.show()\n",
    "fig = px.box(df, y=\"NumberOfTime60-89DaysPastDueNotWorse\", x=\"SeriousDlqin2yrs\")\n",
    "fig.update_layout(yaxis_range=[0,20])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=np.log1p(df[\"NumberOfTimes90DaysLate\"]), color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "\n",
    "fig.show()\n",
    "fig = px.box(df, y=\"NumberOfTimes90DaysLate\", x=\"SeriousDlqin2yrs\")\n",
    "fig.update_layout(yaxis_range=[0,20])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['NumberOfTime30-59DaysPastDueNotWorse'].value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['NumberOfTime60-89DaysPastDueNotWorse'].value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['NumberOfTimes90DaysLate'].value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (dataFrame['NumberOfOpenCreditLinesAndLoans'] == 0) & (dataFrame['NumberRealEstateLoansOrLines'] == 0)\n",
    "\n",
    "dataFrame['NumberOfTimes90DaysLate'] = (\n",
    "    \n",
    "    dataFrame['NumberOfTimes90DaysLate']\n",
    "    .apply(lambda x: int(dataFrame[mask]['NumberOfTimes90DaysLate'].median()) if x >= 96 else x)\n",
    "    \n",
    ")\n",
    "mask = (dataFrame['NumberOfOpenCreditLinesAndLoans'] == 0) & (dataFrame['NumberRealEstateLoansOrLines'] == 0)\n",
    "\n",
    "dataFrame['NumberOfTime60-89DaysPastDueNotWorse'] = (\n",
    "    \n",
    "    dataFrame['NumberOfTime60-89DaysPastDueNotWorse']\n",
    "    .apply(lambda x: int(dataFrame[mask]['NumberOfTime60-89DaysPastDueNotWorse'].median()) if x >= 96 else x)\n",
    "    \n",
    ")\n",
    "mask = (dataFrame['NumberOfOpenCreditLinesAndLoans'] == 0) & (dataFrame['NumberRealEstateLoansOrLines'] == 0)\n",
    "\n",
    "dataFrame['NumberOfTime30-59DaysPastDueNotWorse'] = (\n",
    "    \n",
    "    dataFrame['NumberOfTimes90DaysLate']\n",
    "    .apply(lambda x: int(dataFrame[mask]['NumberOfTime30-59DaysPastDueNotWorse'].median()) if x >= 96 else x)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see there were duplicate values so we had to get rid of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debt Ratio  and Monthly Income Feature\n",
    "\n",
    "The next set of features we would like look at micro level is Debt Ratio, ideally it should be between 0-1 as it is a ratio, but in this context we are receiving values as high as 300k. So what that really is and how it affects the dataset lets see.\n",
    "\n",
    "At the same time the Monthly Income Feature has most missing values, about 29000 missing values which makes about 20% of the dataset. We will take a deeper look into that as well as try and clean it together and see what relationship they share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=np.log1p(df['DebtRatio']), color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "fig.show()\n",
    "\n",
    "fig = px.box(df, y=\"DebtRatio\", x=\"SeriousDlqin2yrs\")\n",
    "fig.update_layout(yaxis_range=[0,2])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph above has a limit set on y-axis because of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['DebtRatio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataFrame['DebtRatio'] > 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About 23% of the Debt Ratio in the whole dataset is greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFrame\n",
    "fig = px.histogram(df, x=np.log1p(df[\"MonthlyIncome\"]), color=\"SeriousDlqin2yrs\",\n",
    "                   marginal=\"violin\", # or violin, rug\n",
    "                   hover_data=df.columns)\n",
    "\n",
    "fig.show()\n",
    "fig = px.box(df, y=\"MonthlyIncome\", x=\"SeriousDlqin2yrs\")\n",
    "fig.update_layout(yaxis_range=[0,20000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph above has a limit set on y-axis because of the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[['MonthlyIncome']].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About 20% of Monthly Values are Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing Monthly Income where debt > 1             : ', round(dataFrame[dataFrame['DebtRatio'] > 1]['MonthlyIncome'].isnull().mean(),2)*100)\n",
    "print('Missing Income in the entire dataset              : ', dataFrame['MonthlyIncome'].isnull().sum())\n",
    "print('Percentage of Missing Income that have a debt > 1 : ',\n",
    "      round((dataFrame[dataFrame['DebtRatio'] > 1]['MonthlyIncome'].isnull().sum()) / (dataFrame['MonthlyIncome'].isnull().sum()),2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 94% of rows with missing \"MonthlyIncome\" have a greater DebtRatio than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[(dataFrame['DebtRatio'] > 1) & (dataFrame['MonthlyIncome'].isnull())]['DebtRatio'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame[(dataFrame['MonthlyIncome'].isnull()) & (dataFrame['DebtRatio'] < 1)]['DebtRatio'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It might be some error with the system, seems like they are hard dollar values, can be either Debt Value instead of Ratio and can be imputed by calcualting Debt Value for all the records and then replacing these with measure of central tendency and then imputating the range of monthly income. As i don't have enough information about it we will just drop the values where Debt Ratio >1 and Monthly Income is NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame=dataFrame[dataFrame['DebtRatio'] < 1]\n",
    "dataFrame=dataFrame[dataFrame['MonthlyIncome'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Possibilities\n",
    "\n",
    "###### When the MonthlyIncome is missing, DebtRatio acquires an abnormal value, which is either equal 0 or greater 1.\n",
    "\n",
    "When Debt Ratio is greater than 0 it may be Debt Value, and when Debt Ratio is zero it might be actual missing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including a Debt Value Feature, which gives Debt in Hard currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame['DebtValue']=dataFrame['MonthlyIncome']*dataFrame['DebtRatio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Data Frame between each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(dataFrame.corr())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Age Bins for better profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['age'] = pd.cut(x=dataFrame['age'], bins=[20, 29, 39, 49, 59, 69, 79, 89, 99, 109])\n",
    "byage_serious = dataFrame.groupby([\"age\",\"SeriousDlqin2yrs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Data on Age bins and Target Variable and observing features how they are, first monthly income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byage_serious[\"MonthlyIncome\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Data on Age bins and Target Variable and observing features how they are, second Debt Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byage_serious[\"DebtRatio\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Data on Age bins and Target Variable and observing features how they are, Third Past Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "byage_serious[\"NumberOfTime30-59DaysPastDueNotWorse\",\"NumberOfTime60-89DaysPastDueNotWorse\",\"NumberOfTimes90DaysLate\"].aggregate(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Data on Age bins and Target Variable and observing features how they are, fourth Debt Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byage_serious[\"DebtValue\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Important Feature Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = dataFrame.drop([\"SeriousDlqin2yrs\"], axis=1)\n",
    "train_y = dataFrame[\"SeriousDlqin2yrs\"].values\n",
    "\n",
    "\n",
    "model = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "## plotando as importÃ¢ncias ##\n",
    "feat_names = train_X.columns.values\n",
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1][:20]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
